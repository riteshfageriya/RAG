# -*- coding: utf-8 -*-
"""RAG_Mindflix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Dfawt09h-utmQ-42UaV_KlVyXi9927y

# Building a RAG application from scratch

This is the high level overview of the system we want to build
"""

from IPython.display import Image, display

display(Image("system1.png"))

import os


os.environ["GEMINI_API_KEY"] = "AIzaSyCcrBaNxQ2hR89WITgIsxHoJZsck8NG650"

print("Updated API Key:", os.environ.get("GEMINI_API_KEY"))

# !pip install python-dotenv
import tempfile
import os

import os
from dotenv import load_dotenv


load_dotenv(".env")


GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

YOUTUBE_VIDEO = "https://www.youtube.com/watch?v=ftDsSB3F5kg"

print("Loaded API Key:", "âœ…" if GEMINI_API_KEY else "âŒ (Key Not Found)")
print("YouTube Video Link:", YOUTUBE_VIDEO)

"""#Setting up the model

Let's define the LLM model that we'll use as part of the workflow.
"""

import google.generativeai as genai

genai.configure(api_key="AIzaSyCcrBaNxQ2hR89WITgIsxHoJZsck8NG650")

model = genai.GenerativeModel("gemini-pro")
response = model.generate_content("Who won the Boarder Gavaskar Trophy Series last time ?")
print(response.text)

"""We can test the model by asking a simple question."""

response=model.generate_content("Provide a detailed answer:What MLB team won the World Series during the COVID-19 pandemic?")

print(response.text)

"""The result from the model happens most of the time  an `AIMessage` instance containing the answer. We can extract this answer by chaining the model with an [output parser](https://python.langchain.com/docs/modules/model_io/output_parsers/).

Here is what chaining the model with an output parser looks like:



For this example, we'll use a simple `StrOutputParser` to extract the answer as a string.
"""

display(Image("chain1.png"))

"""## Introducing prompt templates

We want to provide the model with some context and the question. [Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start) are a simple way to define and reuse prompts.
"""

from langchain.prompts import ChatPromptTemplate

# Define the template with clear instructions
template = """
Answer the question based on the context below. If you can't
answer the question, reply "I don't know".

Context: {context}

Question: {question}
"""

# Create the ChatPromptTemplate from the template
prompt_template = ChatPromptTemplate.from_template(template)

# Format the prompt with specific context and question
formatted_prompt = prompt_template.format(context="Mary's sister is Susana", question="Who is Mary's sister?")

# Print the formatted prompt
print(formatted_prompt)

"""We can now chain the prompt with the model and the output parser."""

display(Image("chain2.png"))

from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseOutputParser
import google.generativeai as genai

genai.configure(api_key="AIzaSyCcrBaNxQ2hR89WITgIsxHoJZsck8NG650")

template = """
Answer the question based on the context below. If you can't
answer the question, reply "I don't know".

Context: {context}

Question: {question}
"""

prompt_template = ChatPromptTemplate.from_template(template)
model = genai.GenerativeModel("gemini-pro")

class SimpleOutputParser(BaseOutputParser):
    def parse(self, text: str) -> str:
        return text.strip()

parser = SimpleOutputParser()

def run_chain(context: str, question: str) -> str:
    formatted_prompt = prompt_template.format(context=context, question=question)
    response = model.generate_content(formatted_prompt)
    parsed_response = parser.parse(response.text)
    return parsed_response

response = run_chain(
    context="Mary's sister is Susana",
    question="Who is Mary's sister?"
)

print(response)

"""## Combining chains

We can combine different chains to create more complex workflows. For example, let's create a second chain that translates the answer from the first chain into a different language.

Let's start by creating a new prompt template for the translation chain:
"""

from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseOutputParser
import google.generativeai as genai

genai.configure(api_key="AIzaSyCcrBaNxQ2hR89WITgIsxHoJZsck8NG650")

answer_template = """
Answer the question based on the context below. If you can't
answer the question, reply "I don't know".

Context: {context}

Question: {question}
"""

translate_template = """
Translate the following text into {language}:

Text: {text}
"""

answer_prompt = ChatPromptTemplate.from_template(answer_template)
translate_prompt = ChatPromptTemplate.from_template(translate_template)
model = genai.GenerativeModel("gemini-pro")

class SimpleOutputParser(BaseOutputParser):
    def parse(self, text: str) -> str:
        return text.strip()

parser = SimpleOutputParser()

def answer_chain(context: str, question: str) -> str:
    formatted_prompt = answer_prompt.format(context=context, question=question)
    response = model.generate_content(formatted_prompt)
    parsed_response = parser.parse(response.text)
    return parsed_response

def translate_chain(text: str, language: str) -> str:
    formatted_prompt = translate_prompt.format(text=text, language=language)
    response = model.generate_content(formatted_prompt)
    parsed_response = parser.parse(response.text)
    return parsed_response

context = "Mary's sister is Susana"
question = "Who is Mary's sister?"
language = "French"

answer = answer_chain(context, question)
translated_answer = translate_chain(answer, language)

print("Answer:", answer)
print("Translated Answer:", translated_answer)

"""We can now create a new translation chain that combines the result from the first chain with the translation prompt.

Here is what the new workflow looks like:
"""

display(Image("chain3.png"))

import time
import google.generativeai as genai
from google.api_core.exceptions import TooManyRequests

# Configure Gemini AI
genai.configure(api_key="your_gemini_api_key_here")

# Initialize the Gemini model
model = genai.GenerativeModel("gemini-pro")

# Define a function to generate answers with retry logic
def generate_answer(context: str, question: str, max_retries: int = 3, delay: int = 2) -> str:
    prompt = f"""
    Answer the question based on the context below. If you can't
    answer the question, reply "I don't know".

    Context: {context}

    Question: {question}
    """
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            return response.text
        except TooManyRequests:
            print(f"Rate limit exceeded. Retrying in {delay} seconds... (Attempt {attempt + 1}/{max_retries})")
            time.sleep(delay)  # Wait before retrying
            delay *= 2  # Exponential backoff
    raise Exception("Failed to generate answer after multiple retries.")

# Define a function to translate text with retry logic
def translate_text(text: str, language: str, max_retries: int = 3, delay: int = 2) -> str:
    prompt = f"""
    Translate the following text into {language}:

    Text: {text}
    """
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            return response.text
        except TooManyRequests:
            print(f"Rate limit exceeded. Retrying in {delay} seconds... (Attempt {attempt + 1}/{max_retries})")
            time.sleep(delay)  # Wait before retrying
            delay *= 2  # Exponential backoff
    raise Exception("Failed to translate text after multiple retries.")

# Define the combined workflow
def combined_workflow(context: str, question: str, language: str) -> str:
    # Step 1: Generate the answer
    answer = generate_answer(context, question)
    print("Generated Answer:", answer)  # Debugging: Print the generated answer

    # Step 2: Translate the answer
    translated_answer = translate_text(answer, language)
    return translated_answer

# Input data
context = "Mary's sister is Susana. She doesn't have any more siblings."
question = "How many sisters does Mary have?"
language = "Spanish"

# Run the combined workflow
try:
    result = combined_workflow(context, question, language)
    print("Translated Answer:", result)
except Exception as e:
    print("Error:", str(e))

"""## Transcribing the YouTube Video

The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using [OpenAI's Whisper](https://openai.com/research/whisper).
"""

# !pip install youtube-transcript-api pytube openai-whisper
# !apt install ffmpeg

import os
import tempfile
from pytube import YouTube
import whisper
from youtube_transcript_api import YouTubeTranscriptApi
from urllib.parse import urlparse, parse_qs

def extract_video_id(url):
    """Smart URL parser for all YouTube formats"""
    if "youtu.be" in url:
        return url.split("/")[-1].split("?")[0]
    return parse_qs(urlparse(url).query).get("v", [None])[0]

def get_english_transcript(video_id):
    """Enhanced transcript fetcher with translation"""
    try:
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

        # Priority 1: Existing English subs
        try:
            transcript = transcript_list.find_transcript(['en'])
            return clean_transcript(transcript.fetch()), "existing_en"
        except:
            pass

        # Priority 2: Translated subs
        for transcript in transcript_list:
            if transcript.is_translatable:
                return clean_transcript(transcript.translate('en').fetch()), "translated"

        # Priority 3: Any available subs
        for transcript in transcript_list:
            return clean_transcript(transcript.fetch()), "raw_subtitle"

        raise Exception("No subtitles found")

    except Exception as e:
        print(f"Subtitle error: {str(e)}")
        return None, "error"

def clean_transcript(entries):
    """Improve subtitle formatting"""
    return " ".join([entry['text'].replace('\n', ' ') for entry in entries])

def transcribe_audio(youtube_url):
    """Robust audio transcription with Whisper"""
    try:
        yt = YouTube(youtube_url)
        audio = yt.streams.filter(only_audio=True).first()

        model = whisper.load_model("small")  # Better accuracy than 'base'

        with tempfile.TemporaryDirectory() as tmpdir:
            audio_path = audio.download(output_path=tmpdir)
            result = model.transcribe(audio_path)
            return result["text"], "audio_transcription"

    except Exception as e:
        return f"Audio failed: {str(e)}", "error"

# Main Execution
youtube_url = "https://www.youtube.com/watch?v=ftDsSB3F5kg"
video_id = extract_video_id(youtube_url)

# Try subtitle methods first
transcript, source = get_english_transcript(video_id)

# Fallback to audio if needed
if not transcript:
    transcript, source = transcribe_audio(youtube_url)

# Save and show results
if transcript and not isinstance(transcript, str):
    with open("transcription.txt", "w") as f:
        f.write(transcript)
    print(f"Success! Source: {source}")
    print("\nSample transcript:")
    print(transcript[:500] + "...")
else:
    print("Failed:", transcript)

"""## Using the entire transcription as context

If we try to invoke the chain using the transcription as context, the model will return an error because the context is too long.

Large Language Models support limitted context sizes. The video we are using is too long for the model to handle, so we need to find a different solution.
"""

display(Image("system2.png"))

# !pip install langchain tiktoken

# !pip install google-generativeai langchain tiktoken

import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tiktoken
import re

# Configure Gemini
genai.configure(api_key="AIzaSyCcrBaNxQ2hR89WITgIsxHoJZsck8NG650")
model = genai.GenerativeModel('gemini-pro')

# 1. Preprocess the transcript
def preprocess_transcript(text):
    # Add newlines after sentence endings
    text = re.sub(r'\. ([A-Z])', r'.\n\1', text)
    # Add paragraph breaks for common video terms
    text = re.sub(r'(\[Music\]|\. )', r'\n\1', text)
    return text

# 2. Load and preprocess transcription
with open("transcription.txt", "r") as f:
    processed_transcript = preprocess_transcript(f.read())

# 3. Configure text splitting for dialogue-heavy content
tokenizer = tiktoken.get_encoding("cl100k_base")

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,  # Smaller chunks for dense content
    chunk_overlap=100,
    length_function=lambda text: len(tokenizer.encode(text)),
    separators=["\n", ". ", "! ", "? ", ", ", " then ", " apart from this", " "]
)

# 4. Split transcript
chunks = text_splitter.split_text(processed_transcript)
print(f"Split into {len(chunks)} meaningful chunks")

# 5. Enhanced processing function
def analyze_chunk(chunk):
    prompt = f"""Analyze this video segment focusing on technical film-making aspects:

    {chunk}

    Identify and explain 2-3 key concepts from this segment related to:
    - Directing techniques
    - Team coordination
    - Technical terminology
    - Production workflow

    Present as bullet points:"""

    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"Analysis error: {str(e)[:150]}"

# 6. Process and save results
with open("analysis.txt", "w") as f:
    for i, chunk in enumerate(chunks):
        f.write(f"\n{'='*40}\nSegment {i+1} Content:\n{chunk}\n\nAnalysis:")
        analysis = analyze_chunk(chunk)
        f.write(f"\n{analysis}\n")

print("Analysis completed with segment breakdown")

"""## Finding the relevant chunks

Given a particular question, we need to find the relevant chunks from the transcription to send to the model. Here is where the idea of **embeddings** comes into play.

An embedding is a mathematical representation of the semantic meaning of a word, sentence, or document. It's a projection of a concept in a high-dimensional space. Embeddings have a simple characteristic: The projection of related concepts will be close to each other, while concepts with different meanings will lie far away. You can use the [Cohere's Embed Playground](https://dashboard.cohere.com/playground/embed) to visualize embeddings in two dimensions.

To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:


Let's generate embeddings for an arbitrary query:
"""

display(Image("system3.png"))

# !pip install cohere google-generativeai tiktoken numpy tenacity streamlit python-dotenv

# Remove all Google Colab imports
# Use standard file handling instead
import tempfile
import os

# Upload the new .env file
# uploaded = files.upload()

import os

# Ensure we keep the correct .env file
if os.path.exists(".env (1)"):
    os.rename(".env (1)", ".env")

print("âœ… .env file is now correctly named.")

import os

keys_to_check = [
    "GEMINI_API_KEY",
    "PINECONE_API_KEY",
    "PINECONE_API_ENV",
    "YOUR_COHERE_API_KEY",
    "YOUR_GOOGLE_API_KEY"
]

for key in keys_to_check:
    value = os.environ.get(key, "ðŸ”´ Not Set")
    print(f"{key}: {value if value != 'ðŸ”´ Not Set' else 'ðŸ”´ Not Set'}")

# !pip install cohere google-generativeai tiktoken numpy tenacity python-dotenv langchain

# core_system.py
import os
import re
import time
import numpy as np
import hashlib
import pickle
from pathlib import Path
from tenacity import retry, wait_exponential, stop_after_attempt
from dotenv import load_dotenv
import cohere
import google.generativeai as genai
from tiktoken import get_encoding
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Configuration Manager
class Config:
    def __init__(self):
        # Initialize default values
        self.chunk_size = 1000
        self.chunk_overlap = 200
        self.embed_model = "embed-english-v3.0"
        self.top_k = 3
        self.cache_dir = Path(".embedding_cache")
        self.rate_limit = 10  # Requests per minute
        self.max_retries = 5
        self.min_chunk_length = 50  # Minimum characters per chunk

        # Load environment variables
        load_dotenv()
        self.cohere_key = os.getenv("YOUR_COHERE_API_KEY")
        self.google_key = os.getenv("GOOGLE_API_KEY")

# Transcript Processor with Enhanced Chunking
class TranscriptProcessor:
    def __init__(self, config):
        self.config = config
        self.enc = get_encoding("cl100k_base")
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            length_function=self._token_counter,
            separators=["\n", ". ", "! ", "? ", "; ", ", ", " then ", "[Music]"]
        )

    def _token_counter(self, text):
        return len(self.enc.encode(text))

    def preprocess(self, text):
        """Clean and normalize transcript text"""
        text = text.replace("\r\n", "\n").strip()
        return re.sub(r"\n{3,}", "\n\n", text)

    def chunk(self, text):
        """Generate context-aware chunks"""
        preprocessed = self.preprocess(text)
        chunks = self.splitter.split_text(preprocessed)
        return [c for c in chunks if len(c) > self.config.min_chunk_length]

# Embedding Service with Smart Caching
class EmbeddingService:
    def __init__(self, config):
        self.config = config
        self.co = cohere.Client(self.config.cohere_key)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    @property
    def cache_dir(self):
        return Path(self.config.cache_dir)

    def _cache_path(self, text):
        return self.cache_dir / f"{hashlib.md5(text.encode()).hexdigest()}.pkl"

    @retry(wait=wait_exponential(multiplier=1, min=4, max=10),
          stop=stop_after_attempt(5))
    def embed(self, texts):
        """Batch embed with persistent caching"""
        cached, to_process = [], []

        for text in texts:
            cache_file = self._cache_path(text)
            if cache_file.exists():
                with open(cache_file, "rb") as f:
                    cached.append(pickle.load(f))
            else:
                to_process.append(text)

        new_embeddings = []
        if to_process:
            response = self.co.embed(
                texts=to_process,
                model=self.config.embed_model,
                input_type="search_document"
            )
            new_embeddings = np.array(response.embeddings)

            for text, emb in zip(to_process, new_embeddings):
                with open(self._cache_path(text), "wb") as f:
                    pickle.dump(emb, f)

        return np.concatenate([cached, new_embeddings]) if cached else new_embeddings

# Semantic Search Engine
class SemanticSearch:
    def __init__(self, processor, embedder, config):
        self.processor = processor
        self.embedder = embedder
        self.config = config
        self.co = cohere.Client(config.cohere_key)

    def find_relevant(self, transcript, query):
        """Find top-k relevant chunks for a query"""
        chunks = self.processor.chunk(transcript)
        if not chunks:
            return []

        chunk_embeddings = self.embedder.embed(chunks)

        # Get query embedding
        query_embed = self.co.embed(
            texts=[query],
            model=self.config.embed_model,
            input_type="search_query"
        ).embeddings[0]

        # Calculate similarities
        similarities = np.dot(chunk_embeddings, query_embed)
        top_indices = np.argsort(similarities)[-self.config.top_k:][::-1]

        return [(chunks[i], float(similarities[i])) for i in top_indices]

# Answer Generator with Rate Limiting
class AnswerGenerator:
    def __init__(self, config):
        self.config = config
        genai.configure(api_key=config.google_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self.last_call = 0

    @retry(wait=wait_exponential(multiplier=1, min=4, max=10),
          stop=stop_after_attempt(5))
    def generate(self, context_chunks, question):
        """Generate answer with rate limiting"""
        self._enforce_rate_limit()
        context = self._format_context(context_chunks)

        prompt = f"""Analyze this video context and answer precisely:
        {context}

        Question: {question}

        Guidelines:
        1. Use markdown formatting
        2. Highlight technical terms in **bold**
        3. Keep under 300 words
        4. Cite relevant sections if available

        Answer:"""

        response = self.model.generate_content(prompt)
        return self._postprocess(response.text)

    def _format_context(self, context_chunks):
        return "\n\n".join(
            f"Context Segment {i+1} (Relevance: {score:.2f}):\n{chunk}"
            for i, (chunk, score) in enumerate(context_chunks)
        )

    def _postprocess(self, text):
        """Clean and format model output"""
        return text.replace("** ", "**").replace(" **", "**")

    def _enforce_rate_limit(self):
        """Token bucket rate limiting"""
        now = time.time()
        elapsed = now - self.last_call
        self.last_call = now
        if elapsed < 60 / self.config.rate_limit:
            time.sleep(60 / self.config.rate_limit - elapsed)

# Unified System Interface
class VideoQASystem:
    def __init__(self):
        self.config = Config()
        self.processor = TranscriptProcessor(self.config)
        self.embedder = EmbeddingService(self.config)
        self.searcher = SemanticSearch(self.processor, self.embedder, self.config)
        self.generator = AnswerGenerator(self.config)

    def process_query(self, transcript, question):
        """End-to-end query processing"""
        relevant_chunks = self.searcher.find_relevant(transcript, question)
        if not relevant_chunks:
            return "No relevant content found in transcript"

        return self.generator.generate(relevant_chunks, question)

# Usage Example
if __name__ == "__main__":
    # Initialize system
    qa_system = VideoQASystem()

    with open("transcription.txt") as f:
      transcript=f.read()

    question="What are the key steps in video production workflow?"
    print(qa_system.process_query(transcript,question))

"""## Setting up a Vector Store

We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a **vector store**.

A vector store is a database of embeddings that specializes in fast similarity searches.



To understand how a vector store works, let's create one in memory and add a few embeddings to it:
"""

display(Image("system4.png"))

# core_system.py
import os
import re
import time
import logging
import numpy as np
import hashlib
import pickle
from pathlib import Path
from collections import defaultdict
from tenacity import retry, wait_exponential, stop_after_attempt
from dotenv import load_dotenv
import cohere
import google.generativeai as genai
from tiktoken import get_encoding
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration Manager (updated for your .env)
class Config:
    def __init__(self):
        self.chunk_size = 1000
        self.chunk_overlap = 200
        self.embed_model = "embed-english-v3.0"
        self.top_k = 3
        self.cache_dir = Path(".embedding_cache")
        self.rate_limit = 10
        self.max_retries = 5
        self.min_chunk_length = 50
        self.embed_dim = 1024  # Cohere v3 dimension

        load_dotenv()
        self.cohere_key = os.getenv("YOUR_COHERE_API_KEY")
        self.google_key = os.getenv("YOUR_GOOGLE_API_KEY")

# Transcript Processor with your filename
class TranscriptProcessor:
    def __init__(self, config):
        self.config = config
        self.enc = get_encoding("cl100k_base")
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=self._token_count,
            separators=["\n", ". ", "! ", "? ", "; ", ", "]
        )

    def _token_count(self, text):
        return len(self.enc.encode(text))

    def preprocess(self, text):
        text = re.sub(r'\s+', ' ', text).strip()
        return re.sub(r'(\[Music\]|\(.*?\))', '', text)

    def chunk(self, text):
        preprocessed = self.preprocess(text)
        chunks = self.splitter.split_text(preprocessed)
        return [c for c in chunks if len(c) > self.config.min_chunk_length]

# Embedding Service with dimension fixes
class EmbeddingService:
    def __init__(self, config):
        self.config = config
        self.co = cohere.Client(config.cohere_key)
        config.cache_dir.mkdir(parents=True, exist_ok=True)

    def _cache_path(self, text):
        return self.config.cache_dir / f"{hashlib.md5(text.encode()).hexdigest()}.pkl"

    @retry(wait=wait_exponential(multiplier=1, min=4, max=10),
          stop=stop_after_attempt(5))
    def embed(self, texts):
        cached = []
        to_process = []

        # Load cached embeddings as 2D arrays
        for text in texts:
            cache_file = self._cache_path(text)
            if cache_file.exists():
                try:
                    with open(cache_file, "rb") as f:
                        emb = pickle.load(f)
                        if emb.ndim == 1:
                            emb = emb.reshape(1, -1)
                        cached.append(emb)
                except Exception as e:
                    logger.warning(f"Cache load error: {e}")
                    to_process.append(text)
            else:
                to_process.append(text)

        # Process new texts
        new_embeddings = np.zeros((0, self.config.embed_dim))
        if to_process:
            try:
                response = self.co.embed(
                    texts=to_process,
                    model=self.config.embed_model,
                    input_type="search_document"
                )
                new_embeddings = np.array(response.embeddings)

                # Save as 2D arrays
                for text, emb in zip(to_process, new_embeddings):
                    with open(self._cache_path(text), "wb") as f:
                        pickle.dump(emb.reshape(1, -1), f)
            except Exception as e:
                logger.error(f"Embedding error: {e}")
                raise

        # Combine embeddings
        if cached:
            cached_embs = np.vstack(cached)
            if new_embeddings.size > 0:
                return np.vstack([cached_embs, new_embeddings])
            return cached_embs
        return new_embeddings

# Vector Store with dimension handling
class VectorStore:
    def __init__(self, embed_dim):
        self.embeddings = np.zeros((0, embed_dim))
        self.chunks = []
        self.index = defaultdict(list)

    def add_documents(self, chunks, embeddings):
        if len(chunks) != embeddings.shape[0]:
            raise ValueError("Chunks/embeddings count mismatch")

        self.embeddings = np.vstack([self.embeddings, embeddings])
        self.chunks.extend(chunks)

        # Build search index
        for idx, chunk in enumerate(chunks):
            for word in set(chunk.lower().split()):
                self.index[word].append(idx)

    def search(self, query_embed, top_k=5):
        if self.embeddings.size == 0:
            return []

        query_embed = np.array(query_embed)
        if query_embed.ndim == 1:
            query_embed = query_embed.reshape(1, -1)

        similarities = np.dot(self.embeddings, query_embed.T).flatten()
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        return [(self.chunks[i], similarities[i]) for i in top_indices]

# Semantic Search Engine
class SemanticSearch:
    def __init__(self, config):
        self.config = config
        self.processor = TranscriptProcessor(config)
        self.embedder = EmbeddingService(config)
        self.vector_store = VectorStore(config.embed_dim)

    def index_transcript(self, transcript):
        chunks = self.processor.chunk(transcript)
        if not chunks:
            raise ValueError("No valid chunks created")

        embeddings = self.embedder.embed(chunks)
        self.vector_store.add_documents(chunks, embeddings)

    def search(self, query):
        try:
            response = self.embedder.co.embed(
                texts=[query],
                model=self.config.embed_model,
                input_type="search_query"
            )
            query_embed = np.array(response.embeddings[0])
            return self.vector_store.search(query_embed, self.config.top_k)
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []

# Answer Generator
class AnswerGenerator:
    def __init__(self, config):
        self.config = config
        genai.configure(api_key=config.google_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self.last_call = 0

    @retry(wait=wait_exponential(multiplier=1, min=4, max=10),
          stop=stop_after_attempt(3))
    def generate(self, context_chunks, question):
        current_time = time.time()
        if current_time - self.last_call < 60/self.config.rate_limit:
            time.sleep(60/self.config.rate_limit - (current_time - self.last_call))

        context = "\n\n".join([f"Context {i+1}: {chunk}"
                             for i, (chunk, _) in enumerate(context_chunks)])
        prompt = f"""Answer using this context:
        {context}

        Question: {question}

        Guidelines:
        - Be specific about video production terms
        - Use bullet points for key items
        - Mention technical roles

        Answer:"""

        try:
            response = self.model.generate_content(prompt)
            self.last_call = time.time()
            return response.text
        except Exception as e:
            logger.error(f"Generation failed: {e}")
            return "Could not generate answer"

# Main System
class VideoQASystem:
    def __init__(self):
        self.config = Config()
        self.search_engine = SemanticSearch(self.config)
        self.answer_gen = AnswerGenerator(self.config)

    def process_transcription(self, file_path="transcription.txt"):
        try:
            with open(file_path, "r") as f:
                transcript = f.read()
            self.search_engine.index_transcript(transcript)
            logger.info("Indexed transcription successfully")
        except Exception as e:
            logger.error(f"Processing failed: {e}")
            raise

    def ask(self, question):
        try:
            results = self.search_engine.search(question)
            if not results:
                return "No relevant information found"
            return self.answer_gen.generate(results, question)
        except Exception as e:
            logger.error(f"Query failed: {e}")
            return "Error processing question"

# Main Execution
if __name__ == "__main__":
    system = VideoQASystem()

    try:
        system.process_transcription()
        print("System ready! Ask about the video content:")

        while True:
            question = input("\nYour question (q to quit): ").strip()
            if question.lower() in ('q', 'quit', 'exit'):
                break
            print(f"\n{system.ask(question)}")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        print("System failed to initialize. Check logs and transcription file.")

display(Image("chain4.png"))

"""Pinecone Integration to scale up our video Q&A system."""

# !pip install pinecone-client

# !pip install cohere google-generativeai tiktoken numpy tenacity python-dotenv langchain pinecone-client

